Multiple sensor systems are extremely used in autonomous driving for providing increased object detection accuracy.We present a multiple sensor based pedestrian detection system that combines aggregated channel features classifiers trained on images captured with two types of sensors: far infrared and stereovision sensors. We developed a spatio-temporal data alignment between the two sensorial systems. For the temporal alignment we used an original camera response timing model for free running cameras in order to align the infrared image with grayscale intensity images captured by trigger-based cameras. The spatial data alignment is done by performing the extrinsic parameters calibration of the infrared camera in the ego car world coordinate system which is also the reference for the stereovision sensors. Based on the aligned sensorial data we developed a classification fusion mechanism for combining infrared and grayscale detections on a unified pedestrian detection stream. We obtain an increased accuracy showing that the two detectors complete each other.
