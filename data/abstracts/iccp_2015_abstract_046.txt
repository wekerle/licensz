4th Generation Long Term Evolution (4G LTE) networks are in commercial service in most countries around the globe, or about to be launched into commercial service. Given that LTE is about to become the global wireless technology of choice, these networks have to operate in any conceivable environment. In doing so, their backhaul transport is served by a wide variety of networks built on a mix of technologies and equipments, carrying the LTE user and signaling traffic. While the performance focus for the deployment of a new wireless network technology is mostly on its own new network elements (NEs) and features, it is equally important to keep in mind that all the NEs of such a network are interconnected by using other network technologies, wired or wireless. These exhibit their own performance characteristics and any impact on their performance can have an impact on the overall network performance. Understanding this complex and interdependent behavior is a key step in understanding how to optimize or even predict performance of 4G LTE networks. And since 4G LTE is fully IP-based, the focus is on understanding and assessing the effect of IP impairments on LTE and its transport links. In this paper, we analyze the impact of backhaul packet loss by focusing on the LTE S1-U interface, which provides user plane transport between the Core Network (CN) and the Evolved NodeBs (eNBs). In order to do so, we emulate different packet loss rates (PLRs) on the S1-U interface, measuring the impact on end user throughput and discussing the aggregated results.
