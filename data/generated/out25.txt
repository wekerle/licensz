Pedestrian recognition using a dynamic modality fusion approach
Adela-Maria Rus, Alexandrina Rogozan, Laura Diosan, Abdelaziz Bensrhair
8
F
pedestrian recognition, fusion, kernel descriptors, SVM
modality pertinence classifier (206), pedestrian recognition (110), pedestrian classifier (100), dynamic modality fusion (95), kernel descriptor (70), non pedestrian (70), modality classifier (70), pedestrian recognition system (63), multi modality (60), dynamic modality fusion model (60), fusion model (60), modality fusion (60), pedestrian detection (50), large feature vector (47), multi modality image (47), modality selection component (47), match kernel (40), pedestrian classification (40), non pedestrian bounding box (40), positive rate (40), majority vote (40), detection rate (40), strong camera calibration setting (40),
Computer Vision
It was proved that the fusion of information from multi-modality images increases the accuracy of pedestrian recognition systems. One of the best approaches so far is to concatenate the features from multi-modality images into a large feature vector, but it requires strong camera calibration settings and non-discriminative modalities could lead to missclassification of some particular images. We present a modality fusion approach for pedestrian recognition, which is able to dynamically select and fuse the most discriminative modalities for a given image and furthermore use them in the classification process. Firstly, we extract kernel descriptor features from a given image in three modalities: intensity, depth and flow. Secondly, we dynamically determine the most suitable modalities for that image using a modality pertinence classifier. Thirdly, we join the features from the selected modalities and classify the image using a linear SVM approach. Numerical experiments are performed on the Daimler benchmark dataset consisting of pedestrian and non-pedestrian bounding boxes captured in outdoor urban environments and indicate that our model outperforms all the individual-modality classifiers and is slightly better than the model obtained by concatenating all multi-modality features.